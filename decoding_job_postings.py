# -*- coding: utf-8 -*-
"""Decoding Job Postings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D_O7p3RlbVw9729DOCeorzhV36MU_3JM
"""

#Dependencies
import pandas as pd
import glob
import numpy as np
from bs4 import BeautifulSoup as bs
from sklearn.feature_extraction.text import CountVectorizer

#Open the html files
all_files = []

files = glob.glob('/content/drive/MyDrive/html_job_postings/*')
for file_name in files:
  with open(file_name, 'r') as f:
    contents = f.read()
    all_files.append(contents)

print(f"Loaded {len(all_files)} HTML files.")

all_html = []

for html in all_files:
  soup = bs(html)
  all_html.append(soup)
print(all_html[1])

posting = {'Title': [], 'Body': []}

for soup in all_html:
  posting['Title'].append(soup.title.text)
  posting['Body'].append(soup.body.text)

df_new = pd.DataFrame(posting)
summary = df_new.describe()
print(summary)

all_bullets = []
for soup in all_html:
  new_bullets = []
  for bullet in soup.find_all('li'):
    new_bullets.append(bullet.text.strip())
  all_bullets.append(new_bullets)

df_new['Bullets'] = all_bullets
df_new['Bullets'] = df_new['Bullets'].apply(tuple, 1)
print(df_new.Bullets)

print(df_new)

df_new.head()

df_new.drop_duplicates(inplace=True)

df_new[df_new['Title'].str.contains('(Data Science)|(Data Scientist)', case = False)]

#df_new.to_csv('step1.csv')
#df_new = pd.read_csv(df_name, converters={'column_name': eval})

resume = open('/content/drive/MyDrive/Liveproject_Resume.pdf', 'r').read()

vectorizer = CountVectorizer()

tf_matrix = vectorizer.fit_transform(newsgroups.data)
print(tf_matrix)