# -*- coding: utf-8 -*-
"""Decoding Job Postings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D_O7p3RlbVw9729DOCeorzhV36MU_3JM
"""

#Dependencies
import pandas as pd
import glob
import numpy as np
from bs4 import BeautifulSoup as bs
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import normalize
from sklearn.cluster import KMeans
from wordcloud import WordCloud
import matplotlib.pyplot as plt

#Open the html files
all_files = []

files = glob.glob('/content/drive/MyDrive/html_job_postings/*')
for file_name in files:
  with open(file_name, 'r') as f:
    contents = f.read()
    all_files.append(contents)

print(f"Loaded {len(all_files)} HTML files.")

all_html = []

for html in all_files:
  soup = bs(html)
  all_html.append(soup)
print(all_html[1])

posting = {'Title': [], 'Body': []}

for soup in all_html:
  posting['Title'].append(soup.title.text)
  posting['Body'].append(soup.body.text)

df_new = pd.DataFrame(posting)
summary = df_new.describe()
print(summary)

all_bullets = []
for soup in all_html:
  new_bullets = []
  for bullet in soup.find_all('li'):
    new_bullets.append(bullet.text.strip())
  all_bullets.append(new_bullets)

df_new['Bullets'] = all_bullets
df_new['Bullets'] = df_new['Bullets'].apply(tuple, 1)
print(df_new.Bullets)

print(df_new)

df_new.head()

df_new.drop_duplicates(inplace=True)
df_new.shape

df_new = df_new[df_new['Title'].str.contains('(Data Science)|(Data Scientist)', case = False)]
df_new

#df_new.to_csv('step1.csv')
#df_new = pd.read_csv(df_name, converters={'column_name': eval})

resume = open('/content/drive/MyDrive/resume.txt', 'r').read()
print(resume)

vectorizer = TfidfVectorizer(stop_words='english')

text = df_new.Body.values.tolist() + [resume]
tf_matrix = vectorizer.fit(text)
tf_matrix = tf_matrix.transform(text)
print(tf_matrix.shape)

num_posts, title_size = tf_matrix.shape
print(f"Our collection of {num_posts} job postings contain a total of "
      f"{title_size} unique titles")

cosine_similarities = cosine_similarity(tf_matrix[:-1] , tf_matrix[-1])
cosine_similarities.shape

df_new['Relevant'] = cosine_similarities
df_sorted = df_new.sort_values(['Relevant'], ascending = False)
df_sorted.reset_index(inplace=True, drop=True)
df_sorted.head(10)

#df_sorted.to_pickle('step2'.pk)

svd_object = TruncatedSVD(n_components=100)
svd_transform_matrix = svd_object.fit_transform(tf_matrix)
svd_transform_matrix.shape

svd_norm_matrix = normalize(svd_transform_matrix)
svd_norm_matrix.shape

svd_cosine_similarity = svd_norm_matrix[:-1] @ svd_norm_matrix[-1].T
svd_cosine_similarity.shape

#k_means_model = KMeans(n_clusters=15)
#clusters = k_means_model.fit_predict(svd_norm_matrix)
#clusters

#df = pd.DataFrame({'Index': range(clusters.size), 'Cluster': clusters})
#df

def compute_cluster_groups(svd_norm_matrix, k=15):
    cluster_model = KMeans(n_clusters=k)
    clusters = cluster_model.fit_predict(svd_norm_matrix)
    df = pd.DataFrame({'Index': range(clusters.size), 'Cluster': clusters})
    return [df_cluster for  _, df_cluster in df.groupby('Cluster')]

cluster_groups = compute_cluster_groups(svd_norm_matrix)
cluster_groups

def cluster_to_image(df_cluster, max_words=10, tf_matrix=tf_matrix,
                     vectorizer=vectorizer):
    indices = df_cluster.Index.values
    summed_tfidf = np.asarray(tf_matrix[indices].sum(axis=0))[0]
    data = {'Word': vectorizer.get_feature_names(),'Summed TFIDF': summed_tfidf}
    df_ranked_words = pd.DataFrame(data).sort_values('Summed TFIDF', ascending=False)
    words_to_score = {word: score
                     for word, score in df_ranked_words[:max_words].values
                     if score != 0}
    cloud_generator = WordCloud(background_color='white',
                                color_func=_color_func,
                                random_state=1)
    wordcloud_image = cloud_generator.fit_words(words_to_score)
    return wordcloud_image

def _color_func(*args, **kwargs):
    return np.random.choice(['black', 'blue', 'teal', 'purple', 'brown'])

wordcloud_image = cluster_to_image(cluster_groups[0])
plt.imshow(wordcloud_image, interpolation="bilinear")
plt.show()

def plot_wordcloud_grid(cluster_groups, num_rows=5, num_columns=3,
                        **kwargs):
    figure, axes = plt.subplots(num_rows, num_columns, figsize=(20, 15))
    cluster_groups_copy = cluster_groups[:]
    for r in range(num_rows):
        for c in range(num_columns):
            if not cluster_groups_copy:
                break

            df_cluster = cluster_groups_copy.pop(0)
            wordcloud_image = cluster_to_image(df_cluster, **kwargs)
            ax = axes[r][c]
            ax.imshow(wordcloud_image,
            interpolation="bilinear")
            ax.set_title(f"Cluster {df_cluster.Cluster.iloc[0]}")
            ax.set_xticks([])
            ax.set_yticks([])

plot_wordcloud_grid(cluster_groups)
plt.show()